{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VdKDqIJGxXo",
        "outputId": "3ca0442b-528d-4868-885a-b61ced78a46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain)\n",
            "  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.81-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.5 langchain-core-0.2.9 langchain-text-splitters-0.2.1 langsmith-0.1.81 orjson-3.10.5\n",
            "Collecting openai\n",
            "  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.3\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.9)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.35.3)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.81)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.1.8 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP4vp6xXpFrN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCyakdVyKbgp"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 객체를 생성합니다.\n",
        "chat_prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chat_model = ChatOpenAI()\n",
        "output_parser = StrOutputParser() # Message의 content를 추출해서 string으로 변환\n",
        "\n",
        "# 체인을 정의합니다. (호출은 아직 하지 않았습니다.)\n",
        "chain = chat_prompt_template | chat_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke 인자로 dict를 넘깁니다.\n",
        "# dict는 key : value 구성되어 있습니다. \"사과\" : \"과일 중 하나이며...\"\n",
        "# 그 이유가 prompt_template이 dict를 받기 때문입니다.\n",
        "\n",
        "chain.invoke({\"topic\": \"ice cream\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6grFdlXFcObi",
        "outputId": "d7c6ffb1-d3e9-430d-aad7-a3e5eacd7e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the ice cream truck break down? Because it had too many \"scoops\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 객체를 따로 생성하지 않고, 체인 정의할 때 바로 생성자를 이용\n",
        "\n",
        "chain = chat_prompt_template | ChatOpenAI() | StrOutputParser()\n",
        "chain.invoke({\"topic\": \"ice cream\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2s8fShzX1Wts",
        "outputId": "4e79b790-0a72-4ee5-b6ee-7e0bf7b52a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the ice cream go to therapy? Because it was feeling a little Rocky Road!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dict말고 \"ice cream\"만 넣고 싶을 때"
      ],
      "metadata": {
        "id": "OUDdNtJIxfa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chat_model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = {\"topic\": RunnablePassthrough()} | chat_prompt_template | chat_model | output_parser"
      ],
      "metadata": {
        "id": "TSEzYgZowP-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke할 때, 굳이 topic 키로 dict로 넣어야 하나? 그냥 문자열로 넣으면 안될까?\n",
        "# 문자열을 받아서, chain에 전달됨\n",
        "# prompt_template의 입력 형식 dict로 받아야한다.\n",
        "# 인자를 value로 dict를 구성해서 넘거야겠다.\n",
        "# \"ice cream\" > RunnablePassthrough() > {\"topic\": RunnablePassthrough()} > prompt_template\n",
        "\n",
        "chain.invoke(\"ice cream\")\n",
        "\n",
        "# RunnablePassthrough() = \"ice cream\"\n",
        "\n",
        "# 질문 : RunnablePassthrough() 형식이 문자열인가요?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jtq6IM4W7hJ7",
        "outputId": "7577df7b-4b14-4bb2-d622-f9de2dbd7ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the ice cream truck break down? Because it had too many \"scoops\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 답 : invoke의 형식을 그대로 따릅니다. 만약 문자열이면 문자열, dict이면 dict입니다.\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chat_model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 이렇게도 가능합니다.\n",
        "chain = RunnablePassthrough() | prompt_template | chat_model | output_parser\n",
        "\n",
        "chain.invoke({\"topic\": \"ice cream\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3VM3a27B8QHh",
        "outputId": "1158b64d-9e9f-4872-f4dc-092fc69c3dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the ice cream truck break down?\\nBecause it had too many \"scoops\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# object approach\n",
        "chain = a.__or__(b)\n",
        "chain(\"some input\")\n",
        "\n",
        "# pipe approach\n",
        "chain = a | b\n",
        "chain(\"some input\")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Y621fdW6dfY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Runnable:\n",
        "\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "\n",
        "    # 이 메서드는 파이썬의 비트 OR 연산자(|)를 오버로드합니다.\n",
        "    def __or__(self, other):\n",
        "\n",
        "        def chained_func(*args, **kwargs):\n",
        "            # the other func consumes the result of this func\n",
        "            return other(self.func(*args, **kwargs))\n",
        "\n",
        "        return Runnable(chained_func)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ],
      "metadata": {
        "id": "eQ7_Yb65bIpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_five(x):\n",
        "    return x + 5\n",
        "\n",
        "def multiply_by_two(x):\n",
        "    return x * 2"
      ],
      "metadata": {
        "id": "OBMlj0tkbM-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap the functions with Runnable\n",
        "add_five = Runnable(add_five)\n",
        "#multiply_by_two = Runnable(multiply_by_two)"
      ],
      "metadata": {
        "id": "KdwcLqVJbOS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chain the runnable functions together\n",
        "chain = add_five | multiply_by_two"
      ],
      "metadata": {
        "id": "6kfPseF3_dBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke the chain\n",
        "chain(3)  # we should return ??"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeFvGwGZ_eVH",
        "outputId": "0e27d45b-412e-418d-895f-a09bd6223c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run them using the object approach\n",
        "chain = add_five.__or__(multiply_by_two)\n",
        "chain(3)  # should return 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg3ciXXqHCCi",
        "outputId": "66cc0654-8d71-463a-88c8-51c0d1cc77e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num2ko(x):\n",
        "    return str(x) + \"입니다.\""
      ],
      "metadata": {
        "id": "WGXEWY7bItV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = add_five | multiply_by_two | num2ko"
      ],
      "metadata": {
        "id": "IOCma1nZI3g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain(3)\n",
        "\n",
        "# num2ko는 Runnable이 아닌데, 호출이 되나요?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VDJt5MYkI4p0",
        "outputId": "71fbf762-a1bb-472c-ec02-21ed7cd7d8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'16입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 템플릿 만드는 방법\n",
        "\n",
        "# 미션 : 어떤 주제에 대해서 사용자 레벨에 맞게 선택된 언어로 대화 예시를 만드는 앱을 개발!\n",
        "# 미션 : [어떤] 주제에 대해서 사용자 [레벨]에 맞게 [선택된 언어]로 대화 예시를 만드는 앱을 개발!\n",
        "\n",
        "# 기획\n",
        "# \"Please generate dialogue sentences in English on the topic of health for a beginner level.\"\n",
        "# \"Please generate dialogue sentences in Korean on the topic of AI for an intermediate level.\"\n",
        "\n",
        "# 양식과 변수를 분리하기\n",
        "# \"Please generate three sentences in a dialogue in (English) on the topic of (health) for a (beginner) level.\"\n",
        "# \"Please generate three sentences in a dialogue in (Korean) on the topic of (AI) for an (intermediate) level.\"\n",
        "\n",
        "# \"Please generate three sentences in a dialogue in {language} on the topic of {topic} for a/an {level} level.\"\n",
        "\n",
        "# 양식 조정하기\n",
        "# \"Please generate three sentences in a dialogue in {language} on the topic of {topic} for a level of {level}.\""
      ],
      "metadata": {
        "id": "mbKP6cxOiQLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Please generate three sentences in a dialogue in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = chat_prompt_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 변수는 dict로 넣습니다.\n",
        "output = chain.invoke({\"language\" : \"English\", \"topic\" : \"travel\", \"level\" : \"beginner\"})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "0CJg_YS9iSSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc782185-7db7-416b-a815-4a9bf7970a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person 1: Where do you want to go on your next trip?\n",
            "Person 2: I want to visit Paris, it's my dream destination.\n",
            "Person 1: That sounds amazing, I hope you have a great time exploring the city of lights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 변수 중에 시스템에서 입력해야할 것과 사용자가 입력해야할 것 분리\n",
        "# language : 설정\n",
        "# topic : 바뀐다.\n",
        "# level : 설정\n",
        "\n",
        "def get_learning_language(_):\n",
        "    print(\"###\")\n",
        "    print(_)\n",
        "    print(\"in get_learning_language\")\n",
        "    print(\"###\")\n",
        "    return \"English\"\n",
        "\n",
        "def get_learning_level(_):\n",
        "    print(\"###\")\n",
        "    print(_)\n",
        "    print(\"in get_learning_level\")\n",
        "    print(\"###\")\n",
        "    return \"beginner\"\n",
        "\n",
        "# 그럼 이 함수를 체인 내에서 어떻게 호출해야하는가?\n",
        "# 예시 ) 휴가 신청서 사용할 때, 신청일시, 이름 등이 자동으로 기입되는 것"
      ],
      "metadata": {
        "id": "WavqPIUFiUrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 방법 1\n",
        "\n",
        "template = \"Please generate three sentences in a dialogue in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt_template | chat_model| StrOutputParser()\n",
        "\n",
        "# invoke 하기 전에 필요한 정보를 dict로 구성하는 방법\n",
        "output = chain.invoke({\"language\" : get_learning_language(''), \"topic\" : \"travel\", \"level\" : get_learning_level('')})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-FI_nBMrUOR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b237b7-cf52-4489-a8e3-e64d1ceb6ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###\n",
            "\n",
            "in get_learning_language\n",
            "###\n",
            "###\n",
            "\n",
            "in get_learning_level\n",
            "###\n",
            "Person 1: Have you ever traveled to another country before?\n",
            "Person 2: Yes, I went to Spain last year. It was amazing!\n",
            "Person 1: I hope to travel abroad someday too. Where would you recommend I visit?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 방법2\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"Please generate three sentences in a dialogue in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# 함수의 인자는 무조건 1개이어야 합니다.\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(language = get_learning_language,\n",
        "                               level = get_learning_level)\n",
        "\n",
        "    | chat_prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# {\"topic\" : \"travel\"} > dict language, level 추가 > {\"topic\", ... \"language\", ... \"level\" ...} dict가 완성\n",
        "# > prompt_template > chat_model\n",
        "\n",
        "output = chain.invoke({\"topic\" : \"travel\"}) # 변수 1개\n",
        "\n",
        "print(output)\n",
        "\n",
        "# 넘어오는 인자에 추가적으로 key와 value를 넣고 싶을 때, RunnablePassthrough을 사용한다."
      ],
      "metadata": {
        "id": "7zoOVeEuiW5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537cb6de-cabe-40ce-d31c-32df29175003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######\n",
            "{'topic': 'travel'}\n",
            "in get_learning_level\n",
            "###\n",
            "\n",
            "{'topic': 'travel'}\n",
            "in get_learning_language\n",
            "###\n",
            "Person A: \"Have you ever been on a plane before?\"\n",
            "Person B: \"No, I haven't. I'm excited to go on my first flight next month.\"\n",
            "Person A: \"That's cool! Where are you going?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "id": "0UCwGveJyklR",
        "outputId": "2f9800ff-0a57-4949-bc3d-d5c64e282be9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableAssign(mapper={\n",
              "  language: RunnableLambda(get_learning_language),\n",
              "  level: RunnableLambda(get_learning_level)\n",
              "})\n",
              "| ChatPromptTemplate(input_variables=['language', 'level', 'topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'level', 'topic'], template='Please generate three sentences in a dialogue in {language} on the topic of {topic} for a level of {level}.'))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x798d78d8f070>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x798d78bf2920>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"Please generate three sentences in a dialogue in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"topic\" : RunnablePassthrough()}\n",
        "    | RunnablePassthrough.assign(language = get_learning_language,\n",
        "                                 level = get_learning_level)\n",
        "\n",
        "    | prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain)\n",
        "\n",
        "output = chain.invoke(\"travel\")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "T90SuR4IiY5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0dae6a3-5954-4e23-f6f5-c7f7544e9037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first={\n",
            "  topic: RunnablePassthrough()\n",
            "} middle=[RunnableAssign(mapper={\n",
            "  language: RunnableLambda(get_learning_language),\n",
            "  level: RunnableLambda(get_learning_level)\n",
            "}), ChatPromptTemplate(input_variables=['language', 'level', 'topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'level', 'topic'], template='Please generate three sentences in a dialogue in {language} on the topic of {topic} for a level of {level}.'))]), ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x798d78d8f070>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x798d78bf2920>, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
            "###\n",
            "{'topic': 'travel'}\n",
            "in get_learning_language\n",
            "###\n",
            "###\n",
            "{'topic': 'travel'}\n",
            "in get_learning_level\n",
            "###\n",
            "Person 1: Have you ever been on a plane before?\n",
            "Person 2: No, I haven't. I usually just drive to go on vacation.\n",
            "Person 1: You should try flying sometime, it's so much faster!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}